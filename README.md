# Awesome-Motion-Diffusion-Models

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)


We collect existing papers on human motion diffusion models published in prominent conferences and journals. 

This paper list will be continuously updated.

## Table of Contents

- [Datasets](#datasets)
- [Survey](#survey)
- [Papers](#papers)
  - [2024](#2024)
  - [2023](#2023)
  - [2022](#2022)
- [Other Resources](#other-resources)

## Datasets
### Text to Motion

- Generating Diverse and Natural 3D Human Motions from Text (**CVPR 2022**) [[project](https://ericguo5513.github.io/text-to-motion/)] [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf)] [[code](https://github.com/EricGuo5513/text-to-motion)]


- BABEL: Bodies, Action and Behavior with English Labels (**CVPR 2021**) [[project](https://babel.is.tue.mpg.de/)] [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Punnakkal_BABEL_Bodies_Action_and_Behavior_With_English_Labels_CVPR_2021_paper.html)] [[code](https://github.com/abhinanda-punnakkal/BABEL)]

- The KIT Motion-Language Dataset (**Big Data 2016**) [[project](https://motion-annotation.humanoids.kit.edu/dataset/)] [[paper](https://matthiasplappert.com/publications/2016_Plappert_Big-Data.pdf)] 

### Audio to Motion

-  AI Choreographer: Music Conditioned 3D Dance Generation with AIST++ (**ICCV 2021**) [[project](https://google.github.io/aichoreographer/)] [[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_AI_Choreographer_Music_Conditioned_3D_Dance_Generation_With_AIST_ICCV_2021_paper.pdf)] [[code](https://github.com/google/aistplusplus_api)]

## Survey

## Papers

### 2024

**CVPR**

- DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion [[project](https://ericguo5513.github.io/momask/)] [[paper](https://arxiv.org/abs/2312.00063)] [[code](https://github.com/EricGuo5513/momask-codes)]
- MMM: Generative Masked Motion Model [[project](https://exitudio.github.io/MMM-page/)] [[paper](http://arxiv.org/abs/2312.03596)] [[code](https://github.com/exitudio/MMM/)]
- FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models [[project](https://shivangi-aneja.github.io/projects/facetalk/)] [[paper](https://arxiv.org/abs/2312.08459)] [[code](https://github.com/shivangi-aneja/FaceTalk)]
- AAMDM: Accelerated Auto-regressive Motion Diffusion Mode [[paper](https://arxiv.org/abs/2401.06146)]
- FlowMDM: Seamless Human Motion Composition with Blended Positional Encodings [[project](https://barquerogerman.github.io/FlowMDM/)] [[paper](https://arxiv.org/abs/2402.15509)] [[code](https://github.com/BarqueroGerman/FlowMDM)]
- OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers [[project](https://tr3e.github.io/omg-page/)] [[paper](https://arxiv.org/abs/2312.08985)] [[code](https://tr3e.github.io/omg-page/)]

**IJCV**

- InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions [[project](https://tr3e.github.io/intergen-page/)] [[paper](https://doi.org/10.1007/s11263-024-02042-6)] [[code](https://github.com/tr3e/InterGen)]

**ICLR**

- SinMDM: Single Motion Diffusion [[project](https://sinmdm.github.io/SinMDM-page/)] [[paper](https://arxiv.org/abs/2302.05905)] [[code](https://github.com/SinMDM/SinMDM)]
- Human Motion Diffusion as a Generative Prior [[project](https://priormdm.github.io/priorMDM-page/)] [[paper](https://arxiv.org/abs/2303.01418)] [[code](https://github.com/priorMDM/priorMDM)]

**arXiv papers**

- DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion [[paper](https://arxiv.org/abs/2309.01372)] [[code](https://github.com/axdfhj/MDD)]
- GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation [[paper](https://arxiv.org/abs/2401.02142)] [[code](https://github.com/xuehao-gao/guess)]
### 2023

**NeurIPS**

- MotionGPT: Human Motion as a Foreign Language [[project](https://motion-gpt.github.io/)] [[paper](https://arxiv.org/abs/2306.14795)] [[code](https://github.com/OpenMotionLab/MotionGPT)]

**ICLR**

- Human Motion Diffusion Model [[project](https://guytevet.github.io/mdm-page/)] [[paper](https://arxiv.org/abs/2209.14916)] [[code](https://github.com/GuyTevet/motion-diffusion-model)]

**ICCV**

- PhysDiff: Physics-Guided Human Motion Diffusion Model [[project](https://nvlabs.github.io/PhysDiff/)] [[paper](https://arxiv.org/abs/2212.02500)]
- GMD: Guided Motion Diffusion for Controllable Human Motion Synthesis [[project](https://korrawe.github.io/gmd-project/)] [[paper](https://arxiv.org/abs/2305.12577)] [[code](https://github.com/korrawe/guided-motion-diffusion)]
- ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model [[project](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html)] [[paper](https://arxiv.org/abs/2304.01116)] [[code](https://github.com/mingyuan-zhang/ReMoDiffuse)]

**CVPR**

- Executing your Commands via Motion Diffusion in Latent Space [[project](https://chenxin.tech/mld/)] [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.pdf)] [[code](https://github.com/chenfengye/motion-latent-diffusion)]
- UDE: A Unified Driving Engine for Human Motion Generation [[project](https://zixiangzhou916.github.io/UDE/)] [[paper](https://arxiv.org/abs/2211.16016)] [[code](https://github.com/zixiangzhou916/UDE)]
- EDGE: Editable Dance Generation From Music [[project](https://edge-dance.github.io/)] [[paper](https://arxiv.org/abs/2211.10658)] [[code](https://github.com/Stanford-TML/EDGE)]
- MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis [[project](https://vcai.mpi-inf.mpg.de/projects/MoFusion/)] [[paper](https://arxiv.org/abs/2212.04495)]
- T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations [[project](https://mael-zys.github.io/T2M-GPT/)] [[paper](https://arxiv.org/abs/2301.06052)] [[code](https://github.com/Mael-zys/T2M-GPT)]

**MMM**

- DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model [[project](https://zf223669.github.io/DiffMotionWebsite/)] [[paper](https://arxiv.org/abs/2301.10047)] [[code](https://github.com/zf223669/DiffmotionGG-beta)]

**ICASSP**

- Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model [[paper](https://ieeexplore.ieee.org/document/10096441)] 

**TOG**

- Listen, denoise, action! Audio-driven motion synthesis with diffusion models [[project](https://www.speech.kth.se/research/listen-denoise-action/)] [[paper](https://arxiv.org/abs/2211.09707)] [[code](https://github.com/simonalexanderson/ListenDenoiseAction)]

**arXiv papers**

- Text2Performer: Text-Driven Human Video Generation [[project](https://yumingj.github.io/projects/Text2Performer.html)] [[paper](https://arxiv.org/abs/2304.08483)] [[code](https://github.com/yumingj/Text2Performer)]

### 2022

**ECCV**

- TEMOS: Generating diverse human motions from textual descriptions [[project](https://mathis.petrovich.fr/temos/)] [[paper](http://arxiv.org/abs/2204.14109)] [[code](https://github.com/Mathux/TEMOS)]

**arXiv papers**

- MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model [[project](https://mingyuan-zhang.github.io/projects/MotionDiffuse.html)] [[paper](https://arxiv.org/abs/2208.15001)] [[code](https://github.com/mingyuan-zhang/MotionDiffuse)]
- FLAME: Free-form Language-based Motion Synthesis & Editing [[project](https://kakaobrain.github.io/flame/)] [[paper](https://arxiv.org/abs/2209.00349)] [[code](https://github.com/kakaobrain/flame)]

## Other Resources

- [fengshiwest/Awesome-Motion-Diffusion-Models](https://github.com/fengshiwest/Awesome-Motion-Diffusion-Models)

## Feel free to contact me if you find any interesting paper is missing.